{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba1874",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy pandas nltk scikit-learn xgboost transformers matplotlib tqdm joblib torch openpyxl tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c8bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, recall_score, roc_curve, precision_recall_curve, auc\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings('ignore')\n",
    "import joblib\n",
    "joblib.parallel_backend('loky', inner_max_num_threads=1)\n",
    "import matplotlib\n",
    "import torch\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# NLTK setup\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69f2048",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Global parameters\n",
    "patient_subset = 1   # 1 - All patients, 2 - Patients with Notes, 3 - Patients without Notes\n",
    "feature_subset = 1   # 1 - Demo, 2 - Demo + Tabular, 3 - Demo + Tabular + Notes\n",
    "feat_sel = 0         # 1 - Feature selection on (RandomForest), 0 = off\n",
    "test_split_ratio = 0.2\n",
    "summarized = 1       # 0 - No summarization, 1 - DeepSeek LLM based summarization\n",
    "seed = 0\n",
    "vectorize_text = 1   # 1 - TF-IDF, 2 - BERT\n",
    "\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "min_max_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b7b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Utility function: Append a single row (dict) to CSV, with an auto-incremented Run ID\n",
    "###############################################################################\n",
    "def append_results_to_csv(results_dict, csv_file):\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(csv_file):\n",
    "        df_existing = pd.read_csv(csv_file)\n",
    "        if 'Run ID' in df_existing.columns:\n",
    "            max_run_id = df_existing['Run ID'].max()\n",
    "        else:\n",
    "\n",
    "            max_run_id = 0\n",
    "        new_run_id = max_run_id + 1\n",
    "        results_dict['Run ID'] = new_run_id\n",
    "            \n",
    "        # Append the new row\n",
    "        df_new = pd.DataFrame([results_dict])\n",
    "        df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_combined.to_csv(csv_file, index=False)\n",
    "    else:\n",
    "        # If no file, create a new DataFrame\n",
    "        results_dict['Run ID'] = 1\n",
    "        df_new = pd.DataFrame([results_dict])\n",
    "        df_new.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fa4269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # include words that satisfy token_pattern=r'[a-zA-Z]{2,}'\n",
    "def filter_tokens_in_notes(notes):\n",
    "    pattern = re.compile(r'[a-zA-Z]{2,}')\n",
    "    filtered_notes = []\n",
    "    for note in notes:\n",
    "        # Find all tokens that match the pattern\n",
    "        filtered_tokens = pattern.findall(note)\n",
    "        # Join tokens back to form the filtered note\n",
    "        filtered_notes.append(' '.join(filtered_tokens))\n",
    "    return filtered_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaf9d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Preprocessing function\n",
    "###############################################################################\n",
    "def preprocess(suhi_df):\n",
    "    suhi_df.dropna(subset=['day_readmit'], inplace=True)\n",
    "    suhi_df.loc[suhi_df['day_readmit'] == 2, 'day_readmit'] = 0\n",
    "    suhi_df['day_readmit'] = suhi_df['day_readmit'].astype(int)                    \n",
    "    return suhi_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "PATIENT_SUBSETS = [1, 2]\n",
    "FEATURE_SUBSETS = [2, 3]\n",
    "ENGAGED_VALUES = [0, 1]\n",
    "SEEDS =  [0, 1] # original value - range(30)\n",
    "SUMMARIZED_OPTIONS = [0, 1]\n",
    "VECTORIZERS = [1,2]  # 1=TF-IDF, 2=BERT [1, 2] \n",
    "FILE_PATH = '../data/suhi_data.xlsx'\n",
    "OUTPUT_CSV = 'training_log.csv'\n",
    "TEST_SPLIT_RATIO = 0.2\n",
    "FEAT_SEL = False\n",
    "N_COMPONENT = 50\n",
    "\n",
    "BEST_PARAMS = {\n",
    "    \"RandomForestClassifier\": { \"n_estimators\": 200, \"max_depth\": 5, \"min_samples_split\": 20 },\n",
    "    \"AdaBoostClassifier\": { \"n_estimators\": 100, \"algorithm\": \"SAMME\", \"learning_rate\": 0.10 },\n",
    "    \"XGBClassifier\": {  \"n_estimators\": 10, \"max_depth\": 5, \"learning_rate\": 0.10 },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess(file_path, patient_subset, engaged, summarized, feature_subset):\n",
    "    df = pd.read_excel(file_path)\n",
    "    df = df[df['engaged'] == engaged]\n",
    "\n",
    "    if patient_subset == 2:\n",
    "        df.dropna(subset=['COMBINED_NOTES'], inplace=True)\n",
    "\n",
    "    if summarized == 1 and feature_subset != 2:\n",
    "        df['FEW_SHORT_LLM_SUMMARY'] = df['FEW_SHORT_LLM_SUMMARY'].replace('nan', '')\n",
    "        df['COMBINED_NOTES'] = df['FEW_SHORT_LLM_SUMMARY']\n",
    "\n",
    "    df = preprocess(df)  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f64dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "# Load BERT and tokenizer once\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model.eval()\n",
    "\n",
    "# Pick device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d1736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(df, feature_subset, vectorize_text, summarized):\n",
    "    min_df = 20 if summarized == 0 else 10\n",
    "    suhi_df = df\n",
    "    text_embeddings = []\n",
    "    \n",
    "    if feature_subset == 3 and vectorize_text == 1:\n",
    "        tfidf_vectorizer = TfidfVectorizer(min_df= min_df, )\n",
    "        suhi_df['COMBINED_NOTES'].fillna('', inplace=True)\n",
    "        suhi_df['COMBINED_NOTES'] = filter_tokens_in_notes(suhi_df['COMBINED_NOTES'])\n",
    "        text_embeddings = tfidf_vectorizer.fit_transform(suhi_df['COMBINED_NOTES'])\n",
    "\n",
    "    # If we include text features and vectorize them using BERT embeddings\n",
    "    if patient_subset == 2 and feature_subset == 3 and vectorize_text == 2:\n",
    "        # Load pre-trained BERT model\n",
    "        suhi_df['COMBINED_NOTES'] = filter_tokens_in_notes(suhi_df['COMBINED_NOTES'])\n",
    "\n",
    "       \n",
    "        # Tokenize and encode the text\n",
    "        for text in tqdm(suhi_df['COMBINED_NOTES'].tolist()):\n",
    "            tokens = tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "            tokens = {key: value.to(device) for key, value in tokens.items()}\n",
    "            with torch.no_grad():\n",
    "                text_embedding = bert_model(**tokens)\n",
    "                text_embeddings.append(text_embedding.pooler_output.cpu().squeeze().numpy())\n",
    "                text_embeddings = np.array(text_embeddings)\n",
    "                pca = PCA().fit(text_embeddings)\n",
    "                pca = PCA(n_components=N_COMPONENT) \n",
    "                text_embeddings = pca.fit_transform(text_embeddings)\n",
    "\n",
    "    # Drop textual/object columns (except for combining them if we do text vectorizing)\n",
    "    text_columns = [col for col in suhi_df.columns if suhi_df[col].dtype == 'object']\n",
    "    suhi_df.drop(columns=text_columns, inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop date columns\n",
    "    date_columns = [col for col in suhi_df.columns if suhi_df[col].dtype == 'datetime64[ns]']\n",
    "    suhi_df.drop(columns=date_columns, inplace=True, errors='ignore')\n",
    "\n",
    "    # Drop columns that contain 'nores'\n",
    "    nores_columns = [col for col in suhi_df.columns if 'nores' in col]\n",
    "    suhi_df.drop(columns=nores_columns, inplace=True, errors='ignore')\n",
    "\n",
    "    # If we have vectorized text, merge them in\n",
    "    if feature_subset == 3 and vectorize_text == 1:\n",
    "        COMBINED_NOTES_vectorized_df = pd.DataFrame(text_embeddings.toarray())\n",
    "        COMBINED_NOTES_vectorized_df.columns = tfidf_vectorizer.get_feature_names_out()\n",
    "        suhi_df.reset_index(drop=True, inplace=True)\n",
    "        suhi_w_vectors_df = pd.concat([suhi_df, COMBINED_NOTES_vectorized_df], axis=1)\n",
    "\n",
    "    elif feature_subset == 3 and vectorize_text == 2:\n",
    "        COMBINED_NOTES_vectorized_df = pd.DataFrame(text_embeddings)\n",
    "        suhi_df.reset_index(drop=True, inplace=True)\n",
    "        suhi_w_vectors_df = pd.concat([suhi_df, COMBINED_NOTES_vectorized_df], axis=1)\n",
    "    else:\n",
    "        suhi_w_vectors_df = suhi_df\n",
    "\n",
    "    suhi_w_vectors_df.columns = suhi_w_vectors_df.columns.astype(str)\n",
    "\n",
    "\n",
    "    # Fill NaN with 0\n",
    "    suhi_w_vectors_df.fillna(0, inplace=True)\n",
    "    return suhi_w_vectors_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a803b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_evaluate_models(X, y, seed):\n",
    "    models = {\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=seed, **BEST_PARAMS[\"RandomForestClassifier\"]),\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=seed, **BEST_PARAMS[\"AdaBoostClassifier\"]),\n",
    "        \"XGBoost\": XGBClassifier(random_state=seed, **BEST_PARAMS[\"XGBClassifier\"]),\n",
    "    }\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT_RATIO, random_state=seed)\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        results[name] = {\n",
    "            \"accuracy\": round(accuracy_score(y_test, y_pred), 4),\n",
    "            \"roc_auc\": round(roc_auc_score(y_test, y_proba), 4),\n",
    "            \"sensitivity\": round(recall_score(y_test, (y_proba >= 0.35).astype(int)), 4),\n",
    "            \"specificity\": round(recall_score(y_test, (y_proba >= 0.35).astype(int), pos_label=0), 4),\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae475d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_subset in PATIENT_SUBSETS:\n",
    "    for feature_subset in FEATURE_SUBSETS:\n",
    "        for engaged in ENGAGED_VALUES:\n",
    "            for seed in SEEDS:\n",
    "                for summarized in SUMMARIZED_OPTIONS:\n",
    "                    for vectorize_option in VECTORIZERS:\n",
    "                        try:\n",
    "                            print(f\"Running: Patient={patient_subset}, Feature={feature_subset}, \"\n",
    "                                  f\"Engaged={engaged}, Seed={seed}, Summarized={summarized}, \"\n",
    "                                  f\"Vectorizer={vectorize_option}\")\n",
    "\n",
    "                            df = load_and_preprocess(FILE_PATH, patient_subset, engaged, summarized, feature_subset)\n",
    "                            df = vectorize_text(df, feature_subset, vectorize_option, summarized)\n",
    "                            X = df.drop('day_readmit', axis=1)\n",
    "                            y = df['day_readmit']\n",
    "\n",
    "\n",
    "                            results = train_and_evaluate_models(X, y, seed)\n",
    "\n",
    "                            final_results = {\n",
    "                                \"Patient Subset\": patient_subset,\n",
    "                                \"Feature Subset\": feature_subset,\n",
    "                                \"Engaged\": engaged,\n",
    "                                \"Seed\": seed,\n",
    "                                \"Summarized\": summarized,\n",
    "                                \"Vectorizer\": vectorize_option,\n",
    "                                **{f\"{clf}_{metric}\": val for clf, metrics in results.items() for metric, val in metrics.items()},\n",
    "                            }\n",
    "\n",
    "                            append_results_to_csv(final_results, OUTPUT_CSV)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error: {e} | Params: {patient_subset, feature_subset, engaged, seed, summarized, vectorize_option}\")\n",
    "                            continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
